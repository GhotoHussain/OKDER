# OKDER
EmoDistil
Optimised Knowledge Distillation for Efficient Social Media Emotion Recognition Using DistilBERT and ALBERT
(DistilBERT with BERT-base as Teacher)
(ALBERT with BERT-base as Teacher)

This repository implements efficient emotion recognition on social media text using knowledge distillation, with:

Teacher Model: BERT-base (high accuracy)

Student Models: Lightweight DistilBERT and ALBERT (optimized for speed).

The code trains student models to mimic the teacherâ€™s predictions, achieving near-BERT performance with fewer parameters, ideal for real-time applications.
